# %% [markdown]
# # LSTM Stock Prediction and Trading Strategy
# 
# This notebook:
# - Predicts mid prices using LSTM
# - Implements a trading strategy based on predictions
# - Backtests using VectorBT

# %% [code]
# Cell 1: Import libraries
import os
import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.layers import Dropout, Activation, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
import vectorbt as vbt

# Set VectorBT settings
vbt.settings.array_wrapper['freq'] = '1s'

# %% [code]
# Cell 2: Define helper functions
def create_trading_decisions(price_data, strategy_type, y_hat_last, start_index):
    decisions = pd.Series(0, index=price_data.index, dtype=int)
    
    if strategy_type == 'momentum':
        test_end_index = start_index + len(y_hat_last)
        if test_end_index > len(price_data):
            test_end_index = len(price_data)
            y_hat_last = y_hat_last[:len(price_data) - start_index]
        
        test_indices = price_data.index[start_index:test_end_index]
        
        prev_prices = price_data['midPrice'].shift(1).loc[test_indices]
        
        buy_signals = y_hat_last > prev_prices.values
        sell_signals = y_hat_last < prev_prices.values
        
        decisions.loc[test_indices] = np.select(
            [buy_signals, sell_signals],
            [1, -1],
            default=0
        )
    
    return decisions

def mean_absolute_percentage_error(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    epsilon = 1e-4
    y_true_safe = tf.clip_by_value(y_true, epsilon, float('inf'))
    
    percentage_errors = tf.abs((y_true_safe - y_pred) / y_true_safe) * 100
    
    max_percentage = 1000.0
    percentage_errors_clipped = tf.clip_by_value(percentage_errors, 0.0, max_percentage)
    
    return tf.reduce_mean(percentage_errors_clipped)

def numpy_mape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    
    epsilon = 1e-6
    y_true_safe = np.clip(y_true, epsilon, None)
    
    percentage_errors = np.abs((y_true_safe - y_pred) / y_true_safe) * 100
    
    max_percentage = 1000.0
    percentage_errors_clipped = np.clip(percentage_errors, 0.0, max_percentage)
    
    return np.mean(percentage_errors_clipped)

def to_sequences(data, seq_len):
    d = []
    for index in range(len(data) - seq_len):
        d.append(data[index: index + seq_len])
    return np.array(d)

def preprocess(data_raw, seq_len, train_split):
    data = to_sequences(data_raw, seq_len)
    num_train = int(train_split * data.shape[0])

    X_train = data[:num_train, :-10, :]
    y_train = data[:num_train, -10, :]

    X_test = data[num_train:, :-10, :]
    y_test = data[num_train:, -10, :]

    return X_train, y_train, X_test, y_test

# %% [code]
# Cell 3: Configuration and setup
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED) 

# Device configuration
device_name = "/device:GPU:0" if tf.config.list_physical_devices('GPU') else "/device:CPU:0"
print(f"Using device: {device_name}")

# %% [code]
# Cell 4: Load and preprocess data
csv_path = "data/2025-08-06-AKBNK-10.csv"
df = pd.read_csv(csv_path, sep=';', parse_dates=['DateTime'])

# Calculate mid price
df['midPrice'] = (df['Level 1 Bid Price'] + df['Level 1 Ask Price']) / 2

# Feature selection
feature_columns = [
    'Depth Ratio',
    'Last Price', 
    'Total Bid Volume',
    ' Total Ask Volume',
    'Level 1 Bid Price',
    'Level 1 Bid Volume',
    'Level 1 Ask Price', 
    'Level 1 Ask Volume',
    'Level 2 Bid Price',
    'Level 2 Bid Volume',
    'Level 2 Ask Price', 
    'Level 2 Ask Volume',
    'Level 3 Bid Price',
    'Level 3 Bid Volume',
    'Level 3 Ask Price', 
    'Level 3 Ask Volume',
    'Level 4 Bid Price',
    'Level 4 Bid Volume',
    'Level 4 Ask Price', 
    'Level 4 Ask Volume',
    'Level 5 Bid Price',
    'Level 5 Bid Volume',
    'Level 5 Ask Price', 
    'Level 5 Ask Volume',
    'midPrice'
]

print(f"Selected Features: {feature_columns}")
print(f"Target Column: midPrice")

# Handle missing values
feature_data = df[feature_columns].values
if np.isnan(feature_data).any():
    print("Warning: NaN values found. Forward-filling them.")
    feature_data = pd.DataFrame(feature_data, columns=feature_columns).fillna(method='ffill').values

# Scale features
scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(feature_data)

# %% [code]
# Cell 5: Prepare sequences
SEQ_LEN = 500
X_train, y_train, X_test, y_test = preprocess(scaled_features, SEQ_LEN, train_split=0.9)

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# %% [code]
# Cell 6: Build and compile model
DROPOUT = 0.2
WINDOW_SIZE = SEQ_LEN - 10
N_FEATURES = scaled_features.shape[1]

model = Sequential([
    LSTM(WINDOW_SIZE, return_sequences=True, input_shape=(WINDOW_SIZE, N_FEATURES)),
    Dropout(DROPOUT),
    LSTM(WINDOW_SIZE, return_sequences=True),
    Dropout(DROPOUT),
    LSTM(WINDOW_SIZE, return_sequences=False),
    Dropout(DROPOUT),
    Dense(N_FEATURES),
    Activation('linear')
])

# Callbacks
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', 
    factor=0.5, 
    patience=3, 
    min_lr=1e-3
)

checkpoint_filepath = 'best_model.h5'
model_checkpoint = ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# Compile model
model.compile(
    loss='mean_squared_error',
    optimizer=Adam(learning_rate=1e-4),
    metrics=[mean_absolute_percentage_error]
)

model.summary()

# %% [code]
# Cell 7: Train the model
BATCH_SIZE = 500
history = model.fit(
    X_train,
    y_train,
    epochs=350,
    batch_size=BATCH_SIZE,
    shuffle=False,
    validation_split=0.2,
    callbacks=[lr_scheduler, model_checkpoint]
)

# %% [code]
# Cell 8: Evaluate model
if os.path.exists(checkpoint_filepath):
    best_model = tf.keras.models.load_model(
        checkpoint_filepath, 
        custom_objects={'mean_absolute_percentage_error': mean_absolute_percentage_error}
    )
else:
    best_model = model

test_loss, test_mape = best_model.evaluate(X_test, y_test, verbose=0)
print(f"Test Loss: {test_loss:.6f}")
print(f"Test MAPE: {test_mape:.2f}%")

# Plot training history
plt.figure(figsize=(12, 4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# %% [code]
# Cell 9: Make predictions
y_hat = best_model.predict(X_test)

# Inverse transform predictions
y_test_inverse = scaler.inverse_transform(y_test)
y_hat_inverse = scaler.inverse_transform(y_hat)

# Extract midPrice predictions
last_col_idx = feature_columns.index('midPrice') 
y_test_last = y_test_inverse[:, last_col_idx]
y_hat_last = y_hat_inverse[:, last_col_idx]

# Plot predictions
plt.figure(figsize=(12, 6))
plt.plot(y_test_last, label="Actual", color='green', alpha=0.7)
plt.plot(y_hat_last, label="Predicted", color='red', alpha=0.7)
plt.title('Mid Price Prediction')
plt.xlabel('Time Steps')
plt.ylabel('Price')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Calculate metrics
mse = np.mean((y_test_last - y_hat_last) ** 2)
mae = np.mean(np.abs(y_test_last - y_hat_last))
rmse = np.sqrt(mse)
mape = numpy_mape(y_test_last, y_hat_last)

print(f"\nPrediction Metrics:")
print(f"MSE: {mse:.6f}")
print(f"MAE: {mae:.6f}")
print(f"RMSE: {rmse:.6f}")
print(f"MAPE: {mape:.2f}%")

# %% [code]
# Cell 10: Backtesting with VectorBT
start_index = SEQ_LEN + int(0.9 * (len(df) - SEQ_LEN))
decisions = create_trading_decisions(df, 'momentum', y_hat_last, start_index)

# Create portfolio
weights = pd.DataFrame(decisions).div(decisions.abs().sum(axis=1), axis=0).fillna(0)
pf = vbt.Portfolio.from_orders(
    close=df['midPrice'],
    size=weights,
    size_type='amount',
    freq='1T',
    init_cash=100,
    cash_sharing=True,
    fees=0.001,
    slippage=0.0005
)

# Print performance metrics
print("\nBacktest Results:")
print(f"Total Return: {pf.total_return():.2%}")
print(f"Sharpe Ratio: {pf.sharpe_ratio():.3f}")
print(f"Max Drawdown: {pf.max_drawdown():.2%}")

# Plot portfolio value
pf.plot().show()