{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Mid-Price Prediction\n",
    "This notebook implements a multivariate LSTM model to predict mid-price movements using order book data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Custom MAPE metric with clipping for stability\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    y_true_safe = tf.clip_by_value(y_true, epsilon, float('inf'))\n",
    "    \n",
    "    percentage_errors = tf.abs((y_true_safe - y_pred) / y_true_safe) * 100\n",
    "    \n",
    "    max_percentage = 1000.0\n",
    "    percentage_errors_clipped = tf.clip_by_value(percentage_errors, 0.0, max_percentage)\n",
    "    \n",
    "    return tf.reduce_mean(percentage_errors_clipped)\n",
    "\n",
    "def numpy_mape(y_true, y_pred):\n",
    "    \"\"\"Numpy implementation of MAPE with debugging output\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    print(f\"MAPE Debug - y_true range: {np.min(y_true):.6f} to {np.max(y_true):.6f}\")\n",
    "    print(f\"MAPE Debug - y_pred range: {np.min(y_pred):.6f} to {np.max(y_pred):.6f}\")\n",
    "    print(f\"MAPE Debug - y_true mean: {np.mean(y_true):.6f}\")\n",
    "    print(f\"MAPE Debug - y_pred mean: {np.mean(y_pred):.6f}\")\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    y_true_safe = np.clip(y_true, epsilon, None)\n",
    "    \n",
    "    percentage_errors = np.abs((y_true_safe - y_pred) / y_true_safe) * 100\n",
    "    \n",
    "    max_percentage = 1000.0\n",
    "    percentage_errors_clipped = np.clip(percentage_errors, 0.0, max_percentage)\n",
    "    \n",
    "    print(f\"MAPE Debug - Percentage errors range: {np.min(percentage_errors_clipped):.2f}% to {np.max(percentage_errors_clipped):.2f}%\")\n",
    "    print(f\"MAPE Debug - Percentage errors mean: {np.mean(percentage_errors_clipped):.2f}%\")\n",
    "    print(f\"MAPE Debug - Number of clipped values: {np.sum(percentage_errors > max_percentage)}\")\n",
    "    \n",
    "    return np.mean(percentage_errors_clipped)\n",
    "\n",
    "def to_sequences(data, seq_len):\n",
    "    \"\"\"Convert 2D array into sequences of specified length\"\"\"\n",
    "    d = []\n",
    "    for index in range(len(data) - seq_len):\n",
    "        d.append(data[index: index + seq_len])\n",
    "    return np.array(d)\n",
    "\n",
    "def preprocess(data_raw, seq_len, train_split):\n",
    "    \"\"\"Preprocess raw data into sequences and split into train/test\"\"\"\n",
    "    data = to_sequences(data_raw, seq_len)\n",
    "    num_train = int(train_split * data.shape[0])\n",
    "\n",
    "    # X will be sequences of length (SEQ_LEN - 10)\n",
    "    # y will be the 10th step after the X sequence ends\n",
    "    X_train = data[:num_train, :-10, :]\n",
    "    y_train = data[:num_train, -10, :]\n",
    "\n",
    "    X_test = data[num_train:, :-10, :]\n",
    "    y_test = data[num_train:, -10, :]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run LSTM model training and evaluation\"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    RANDOM_SEED = 42\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.random.set_seed(RANDOM_SEED) \n",
    "\n",
    "    # Check GPU availability\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"\\nTensorFlow detected {len(gpus)} GPU(s):\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"  - {gpu}\")\n",
    "    else:\n",
    "        print(\"\\nNo GPU detected. Running on CPU.\")\n",
    " \n",
    "    # Load and prepare data\n",
    "    csv_path = \"data/2025-07-01-AKBNK-5.csv\"\n",
    "    df = pd.read_csv(csv_path, sep=';', parse_dates=['DateTime'])\n",
    "    \n",
    "    # Calculate mid price\n",
    "    df['midPrice'] = (df['Level 1 Bid Price'] + df['Level 1 Ask Price']) / 2\n",
    "    \n",
    "    # Define features and target\n",
    "    feature_columns = [\n",
    "        'Depth Ratio',\n",
    "        'Last Price', \n",
    "        'Total Bid Volume',\n",
    "        ' Total Ask Volume',\n",
    "        'Level 1 Bid Price',\n",
    "        'Level 1 Bid Volume',\n",
    "        'Level 1 Ask Price', \n",
    "        'Level 1 Ask Volume',\n",
    "        'Level 2 Bid Price',\n",
    "        'Level 2 Bid Volume',\n",
    "        'Level 2 Ask Price', \n",
    "        'Level 2 Ask Volume',\n",
    "        'Level 3 Bid Price',\n",
    "        'Level 3 Bid Volume',\n",
    "        'Level 3 Ask Price', \n",
    "        'Level 3 Ask Volume',\n",
    "        'Level 4 Bid Price',\n",
    "        'Level 4 Bid Volume',\n",
    "        'Level 4 Ask Price', \n",
    "        'Level 4 Ask Volume',\n",
    "        'Level 5 Bid Price',\n",
    "        'Level 5 Bid Volume',\n",
    "        'Level 5 Ask Price', \n",
    "        'Level 5 Ask Volume',\n",
    "        'midPrice'\n",
    "    ]\n",
    "    \n",
    "    target_column = 'midPrice'\n",
    "    \n",
    "    print(f\"\\nSelected Features: {feature_columns}\")\n",
    "    print(f\"Target Column: {target_column}\")\n",
    "    \n",
    "    # Prepare feature data\n",
    "    feature_data = df[feature_columns].values\n",
    "    \n",
    "    if np.isnan(feature_data).any():\n",
    "        print(\"Warning: NaN values found in feature data. Forward-filling them.\")\n",
    "        feature_data = pd.DataFrame(feature_data, columns=feature_columns).fillna(method='ffill').values\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Create sequences\n",
    "    SEQ_LEN = 150\n",
    "    X_train, y_train, X_test, y_test = preprocess(scaled_features, SEQ_LEN, train_split=0.9)\n",
    "    \n",
    "    # Model architecture\n",
    "    DROPOUT = 0.2\n",
    "    WINDOW_SIZE = SEQ_LEN - 1\n",
    "    N_FEATURES = scaled_features.shape[1]\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        LSTM(WINDOW_SIZE, return_sequences=True, input_shape=(WINDOW_SIZE, N_FEATURES)),\n",
    "        Dropout(DROPOUT),\n",
    "        LSTM(WINDOW_SIZE, return_sequences=True),\n",
    "        Dropout(rate=DROPOUT),\n",
    "        LSTM(WINDOW_SIZE, return_sequences=False), \n",
    "        Dropout(rate=DROPOUT),\n",
    "        Dense(units=N_FEATURES),\n",
    "        Activation('linear')\n",
    "    ])\n",
    "\n",
    "    # Callbacks\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        min_lr=1e-3\n",
    "    )\n",
    "    \n",
    "    checkpoint_filepath = 'best_model.h5'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    adam = Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        loss='mean_squared_error', \n",
    "        optimizer=adam, \n",
    "        metrics=[mean_absolute_percentage_error]\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    BATCH_SIZE = 500\n",
    "    print(\"\\nStarting model training...\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False, \n",
    "        validation_split=0.2,\n",
    "        callbacks=[lr_scheduler, model_checkpoint_callback]\n",
    "    )\n",
    "    print(\"Model training finished.\")\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    if os.path.exists(checkpoint_filepath):\n",
    "        print(f\"\\nLoading best model from {checkpoint_filepath} for evaluation.\")\n",
    "        best_model = keras.models.load_model(checkpoint_filepath, \n",
    "                                        custom_objects={'mean_absolute_percentage_error': mean_absolute_percentage_error})\n",
    "    else:\n",
    "        print(f\"\\nError: Best model not found at {checkpoint_filepath}. Using the last trained model.\")\n",
    "        best_model = model\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model on test data...\")\n",
    "    test_loss, test_mape = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss (best model): {test_loss:.6f}\")\n",
    "    print(f\"Test MAPE (best model): {test_mape:.2f}%\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss Over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions on test data...\")\n",
    "    y_hat = best_model.predict(X_test)\n",
    "\n",
    "    # Inverse transform predictions\n",
    "    y_test_inverse = scaler.inverse_transform(y_test)\n",
    "    y_hat_inverse = scaler.inverse_transform(y_hat)\n",
    "    \n",
    "    # Extract midPrice predictions\n",
    "    last_col_idx = feature_columns.index('midPrice') \n",
    "    y_test_last = y_test_inverse[:, last_col_idx]\n",
    "    y_hat_last = y_hat_inverse[:, last_col_idx]\n",
    "\n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test_last, label=\"Actual Mid Price\", color='green', alpha=0.7)\n",
    "    plt.plot(y_hat_last, label=\"Predicted Mid Price\", color='red', alpha=0.7)\n",
    "\n",
    "    plt.title('Mid Price Prediction - Multivariate LSTM')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = np.mean((y_test_last - y_hat_last) ** 2)\n",
    "    mae = np.mean(np.abs(y_test_last - y_hat_last))\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = numpy_mape(y_test_last, y_hat_last)\n",
    "\n",
    "    print(f\"\\nPrediction Metrics (on inverse transformed 'midPrice'):\")\n",
    "    print(f\"MSE: {mse:.6f}\")\n",
    "    print(f\"MAE: {mae:.6f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nFeature columns used for training: {feature_columns}\")\n",
    "    print(f\"Number of features: {N_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
