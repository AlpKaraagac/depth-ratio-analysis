{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Trading Strategy Implementation\n",
    "This notebook implements an LSTM-based trading strategy using TensorFlow/Keras and VectorBT for backtesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install vectorbt scikit-learn tensorflow seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Configure settings\n",
    "vbt.settings.array_wrapper['freq'] = '1s'\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trading_decisions(price_data, strategy_type, y_hat_last, start_index):\n",
    "    \"\"\"Create trading decisions based on momentum strategy.\"\"\"\n",
    "    decisions = pd.Series(0, index=price_data.index, dtype=int)\n",
    "    \n",
    "    if strategy_type == 'momentum':\n",
    "        test_end_index = start_index + len(y_hat_last)\n",
    "        if test_end_index > len(price_data):\n",
    "            test_end_index = len(price_data)\n",
    "            y_hat_last = y_hat_last[:len(price_data) - start_index]\n",
    "        \n",
    "        test_indices = price_data.index[start_index:test_end_index]\n",
    "        prev_prices = price_data['midPrice'].shift(1).loc[test_indices]\n",
    "        \n",
    "        buy_signals = y_hat_last > prev_prices.values\n",
    "        sell_signals = y_hat_last < prev_prices.values\n",
    "        \n",
    "        decisions.loc[test_indices] = np.select(\n",
    "            [buy_signals, sell_signals],\n",
    "            [1, -1],\n",
    "            default=0\n",
    "        )\n",
    "    \n",
    "    return decisions\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Custom MAPE metric implementation for TensorFlow.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    y_true_safe = tf.clip_by_value(y_true, epsilon, float('inf'))\n",
    "    \n",
    "    percentage_errors = tf.abs((y_true_safe - y_pred) / y_true_safe) * 100\n",
    "    max_percentage = 1000.0\n",
    "    percentage_errors_clipped = tf.clip_by_value(percentage_errors, 0.0, max_percentage)\n",
    "    \n",
    "    return tf.reduce_mean(percentage_errors_clipped)\n",
    "\n",
    "def numpy_mape(y_true, y_pred):\n",
    "    \"\"\"Custom MAPE implementation for NumPy with debugging info.\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    print(f\"MAPE Debug - y_true range: {np.min(y_true):.6f} to {np.max(y_true):.6f}\")\n",
    "    print(f\"MAPE Debug - y_pred range: {np.min(y_pred):.6f} to {np.max(y_pred):.6f}\")\n",
    "    print(f\"MAPE Debug - y_true mean: {np.mean(y_true):.6f}\")\n",
    "    print(f\"MAPE Debug - y_pred mean: {np.mean(y_pred):.6f}\")\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    y_true_safe = np.clip(y_true, epsilon, None)\n",
    "    \n",
    "    percentage_errors = np.abs((y_true_safe - y_pred) / y_true_safe) * 100\n",
    "    max_percentage = 1000.0\n",
    "    percentage_errors_clipped = np.clip(percentage_errors, 0.0, max_percentage)\n",
    "    \n",
    "    print(f\"MAPE Debug - Percentage errors range: {np.min(percentage_errors_clipped):.2f}% to {np.max(percentage_errors_clipped):.2f}%\")\n",
    "    print(f\"MAPE Debug - Percentage errors mean: {np.mean(percentage_errors_clipped):.2f}%\")\n",
    "    print(f\"MAPE Debug - Number of clipped values: {np.sum(percentage_errors > max_percentage)}\")\n",
    "    \n",
    "    return np.mean(percentage_errors_clipped)\n",
    "\n",
    "def to_sequences(data, seq_len):\n",
    "    \"\"\"Convert 2D array into sequences of specified length.\"\"\"\n",
    "    d = []\n",
    "    for index in range(len(data) - seq_len):\n",
    "        d.append(data[index: index + seq_len])\n",
    "    return np.array(d)\n",
    "\n",
    "def preprocess(data_raw, seq_len, train_split):\n",
    "    \"\"\"Preprocess data into sequences and split into train/test sets.\"\"\"\n",
    "    data = to_sequences(data_raw, seq_len)\n",
    "    num_train = int(train_split * data.shape[0])\n",
    "\n",
    "    # X: sequences of length (SEQ_LEN - 10)\n",
    "    # y: 10th step after the X sequence ends\n",
    "    X_train = data[:num_train, :-10, :]\n",
    "    y_train = data[:num_train, -10, :]\n",
    "\n",
    "    X_test = data[num_train:, :-10, :]\n",
    "    y_test = data[num_train:, -10, :]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # GPU Configuration\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU detected - configuring...\")\n",
    "        try:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                tf.config.list_physical_devices('GPU')[0],\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=1024)]\n",
    "            )\n",
    "            device_name = \"/device:GPU:0\"\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "            device_name = \"/device:CPU:0\"\n",
    "    else:\n",
    "        print(\"No GPU detected - using CPU\")\n",
    "        device_name = \"/device:CPU:0\"\n",
    "        \n",
    "    print(f\"Using device: {device_name}\")\n",
    "\n",
    "    # Load and prepare data\n",
    "    csv_path = \"data/2025-08-06-AKBNK-10.csv\"\n",
    "    df = pd.read_csv(csv_path, sep=';', parse_dates=['DateTime'])\n",
    "    \n",
    "    # Feature engineering\n",
    "    df['midPrice'] = (df['Level 1 Bid Price'] + df['Level 1 Ask Price']) / 2\n",
    "    \n",
    "    feature_columns = [\n",
    "        'Depth Ratio', 'Last Price', 'Total Bid Volume', ' Total Ask Volume',\n",
    "        'Level 1 Bid Price', 'Level 1 Bid Volume', 'Level 1 Ask Price', 'Level 1 Ask Volume',\n",
    "        'Level 2 Bid Price', 'Level 2 Bid Volume', 'Level 2 Ask Price', 'Level 2 Ask Volume',\n",
    "        'Level 3 Bid Price', 'Level 3 Bid Volume', 'Level 3 Ask Price', 'Level 3 Ask Volume',\n",
    "        'Level 4 Bid Price', 'Level 4 Bid Volume', 'Level 4 Ask Price', 'Level 4 Ask Volume',\n",
    "        'Level 5 Bid Price', 'Level 5 Bid Volume', 'Level 5 Ask Price', 'Level 5 Ask Volume',\n",
    "        'midPrice'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nSelected {len(feature_columns)} features\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    feature_data = df[feature_columns].values\n",
    "    if np.isnan(feature_data).any():\n",
    "        print(\"NaN values found - forward filling...\")\n",
    "        feature_data = pd.DataFrame(feature_data, columns=feature_columns).fillna(method='ffill').values\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Sequence preprocessing\n",
    "    SEQ_LEN = 300\n",
    "    X_train, y_train, X_test, y_test = preprocess(scaled_features, SEQ_LEN, train_split=0.9)\n",
    "    \n",
    "    print(f\"\\nData shapes:\\nX_train: {X_train.shape}\\ny_train: {y_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\\ny_test: {y_test.shape}\")\n",
    "    \n",
    "    # Model architecture\n",
    "    DROPOUT = 0.2\n",
    "    WINDOW_SIZE = SEQ_LEN - 10\n",
    "    N_FEATURES = scaled_features.shape[1]\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(WINDOW_SIZE, return_sequences=True, input_shape=(WINDOW_SIZE, N_FEATURES)),\n",
    "        Dropout(DROPOUT),\n",
    "        LSTM(WINDOW_SIZE, return_sequences=True),\n",
    "        Dropout(DROPOUT),\n",
    "        LSTM(WINDOW_SIZE, return_sequences=False),\n",
    "        Dropout(DROPOUT),\n",
    "        Dense(N_FEATURES),\n",
    "        Activation('linear')\n",
    "    ])\n",
    "    \n",
    "    # Callbacks\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        min_lr=1e-3\n",
    "    )\n",
    "    \n",
    "    checkpoint_filepath = 'best_model.h5'\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Compile and train\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        metrics=[mean_absolute_percentage_error]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining model...\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=350,\n",
    "        batch_size=500,\n",
    "        shuffle=False,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[lr_scheduler, model_checkpoint]\n",
    "    )\n",
    "    \n",
    "    # Load best model\n",
    "    if os.path.exists(checkpoint_filepath):\n",
    "        best_model = keras.models.load_model(\n",
    "            checkpoint_filepath, \n",
    "            custom_objects={'mean_absolute_percentage_error': mean_absolute_percentage_error}\n",
    "        )\n",
    "    else:\n",
    "        best_model = model\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_mape = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Metrics:\\nLoss: {test_loss:.4f}\\nMAPE: {test_mape:.2f}%\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_hat = best_model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    y_test_inverse = scaler.inverse_transform(y_test)\n",
    "    y_hat_inverse = scaler.inverse_transform(y_hat)\n",
    "    \n",
    "    # Extract midPrice predictions\n",
    "    mid_price_idx = feature_columns.index('midPrice')\n",
    "    y_test_mid = y_test_inverse[:, mid_price_idx]\n",
    "    y_hat_mid = y_hat_inverse[:, mid_price_idx]\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test_mid, label=\"Actual\", color='green', alpha=0.7)\n",
    "    plt.plot(y_hat_mid, label=\"Predicted\", color='red', alpha=0.7)\n",
    "    plt.title('Mid Price Predictions')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = np.mean((y_test_mid - y_hat_mid) ** 2)\n",
    "    mae = np.mean(np.abs(y_test_mid - y_hat_mid))\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape_val = numpy_mape(y_test_mid, y_hat_mid)\n",
    "    \n",
    "    print(f\"\\nPrediction Metrics:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape_val:.2f}%\")\n",
    "    \n",
    "    # Generate trading signals\n",
    "    start_idx = SEQ_LEN + int(0.9 * (len(df) - SEQ_LEN))\n",
    "    decisions = create_trading_decisions(df, 'momentum', y_hat_mid, start_idx)\n",
    "    \n",
    "    # Backtest with VectorBT\n",
    "    print(\"\\nRunning backtest...\")\n",
    "    weights = pd.DataFrame(decisions).div(pd.DataFrame(decisions).abs().sum(axis=1), axis=0).fillna(0)\n",
    "    \n",
    "    pf = vbt.Portfolio.from_orders(\n",
    "        close=df['midPrice'],\n",
    "        size=weights,\n",
    "        size_type='targetpercent',\n",
    "        freq='1s',\n",
    "        init_cash=100,\n",
    "        cash_sharing=True\n",
    "    )\n",
    "    \n",
    "    # Display backtest results\n",
    "    orders = pf.orders\n",
    "    print(f\"\\nTrades executed:\\nBuy: {orders.buy.count()}\\nSell: {orders.sell.count()}\")\n",
    "    \n",
    "    full_stats = pf.stats()\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(f\"Total Return: {full_stats['Total Return [%]']:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {full_stats['Sharpe Ratio']:.2f}\")\n",
    "    print(f\"Max Drawdown: {full_stats['Max Drawdown [%]']:.2f}%\")\n",
    "    \n",
    "    # Plot portfolio value\n",
    "    pf.value().plot(title='Portfolio Value')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show detailed portfolio plot\n",
    "    pf.plot().show()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}