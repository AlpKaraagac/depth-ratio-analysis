{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow LSTM Model for Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pickle\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Configure TensorFlow to ignore oneDNN optimizations and select device\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"\\nGPU detected. Attempting to use...\")\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            tf.config.list_physical_devices('GPU')[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=1024)]\n",
    "        )\n",
    "        device_name = \"/device:GPU:0\"\n",
    "        print(\"Using GPU.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        device_name = \"/device:CPU:0\"\n",
    "        print(\"Failed to set GPU memory, falling back to CPU.\")\n",
    "else:\n",
    "    print(\"\\nNo GPU detected. Using CPU.\")\n",
    "    device_name = \"/device:CPU:0\"\n",
    "    \n",
    "print(f\"Using device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Custom Keras MAPE metric.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    y_true_safe = tf.clip_by_value(y_true, epsilon, float('inf'))\n",
    "    \n",
    "    percentage_errors = tf.abs((y_true_safe - y_pred) / y_true_safe) * 100\n",
    "    \n",
    "    max_percentage = 1000.0\n",
    "    percentage_errors_clipped = tf.clip_by_value(percentage_errors, 0.0, max_percentage)\n",
    "    \n",
    "    return tf.reduce_mean(percentage_errors_clipped)\n",
    "\n",
    "def to_sequences(data, seq_len):\n",
    "    \"\"\"\n",
    "    Converts a 2D array into sequences of a specified length.\n",
    "    Each sequence will have `seq_len` time steps.\n",
    "    \"\"\"\n",
    "    d = []\n",
    "    for index in range(len(data) - seq_len):\n",
    "        d.append(data[index: index + seq_len])\n",
    "    return np.array(d)\n",
    "\n",
    "def preprocess(data_raw, seq_len, train_split):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw data into sequences and splits it into\n",
    "    training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        data_raw (np.array): The raw input data.\n",
    "        seq_len (int): The length of each sequence.\n",
    "        train_split (float): The proportion of data to use for training.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    data = to_sequences(data_raw, seq_len)\n",
    "    num_train = int(train_split * data.shape[0])\n",
    "\n",
    "    # X will be sequences of length (SEQ_LEN - 10)\n",
    "    # y will be the 10th step after the X sequence ends\n",
    "    X_train = data[:num_train, :-10, :]\n",
    "    y_train = data[:num_train, -10, :]\n",
    "\n",
    "    X_test = data[num_train:, :-10, :]\n",
    "    y_test = data[num_train:, -10, :]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def numpy_mape(y_true, y_pred):\n",
    "    \"\"\"A NumPy-based MAPE for final evaluation.\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    print(f\"MAPE Debug - y_true range: {np.min(y_true):.6f} to {np.max(y_true):.6f}\")\n",
    "    print(f\"MAPE Debug - y_pred range: {np.min(y_pred):.6f} to {np.max(y_pred):.6f}\")\n",
    "    print(f\"MAPE Debug - y_true mean: {np.mean(y_true):.6f}\")\n",
    "    print(f\"MAPE Debug - y_pred mean: {np.mean(y_pred):.6f}\")\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    y_true_safe = np.clip(y_true, epsilon, None)\n",
    "    \n",
    "    percentage_errors = np.abs((y_true_safe - y_pred) / y_true_safe) * 100\n",
    "    \n",
    "    max_percentage = 1000.0\n",
    "    percentage_errors_clipped = np.clip(percentage_errors, 0.0, max_percentage)\n",
    "    \n",
    "    print(f\"MAPE Debug - Percentage errors range: {np.min(percentage_errors_clipped):.2f}% to {np.max(percentage_errors_clipped):.2f}%\")\n",
    "    print(f\"MAPE Debug - Percentage errors mean: {np.mean(percentage_errors_clipped):.2f}%\")\n",
    "    print(f\"MAPE Debug - Number of clipped values: {np.sum(percentage_errors > max_percentage)}\")\n",
    "    \n",
    "    return np.mean(percentage_errors_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Data Loading and Preprocessing ---\n",
    "    data_dir = \"data\"\n",
    "    csv_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.csv')])\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {file}\")\n",
    "    \n",
    "    dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(data_dir, csv_file)\n",
    "        print(f\"Reading {csv_file}...\")\n",
    "        temp_df = pd.read_csv(csv_path, sep=';', parse_dates=['DateTime'])\n",
    "        dfs.append(temp_df)\n",
    "        print(f\"  - Shape: {temp_df.shape}\")\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nCombined DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    df = df.sort_values('DateTime').reset_index(drop=True)\n",
    "    print(f\"DataFrame sorted by DateTime, final shape: {df.shape}\")\n",
    "\n",
    "    df['midPrice'] = (df['Level 1 Bid Price'] + df['Level 1 Ask Price']) / 2\n",
    "    \n",
    "    feature_columns = [\n",
    "        'Depth Ratio',\n",
    "        'Last Price', \n",
    "        'Total Bid Volume',\n",
    "        ' Total Ask Volume',\n",
    "        'Level 1 Bid Price',\n",
    "        'Level 1 Bid Volume',\n",
    "        'Level 1 Ask Price', \n",
    "        'Level 1 Ask Volume',\n",
    "        'Level 2 Bid Price',\n",
    "        'Level 2 Bid Volume',\n",
    "        'Level 2 Ask Price', \n",
    "        'Level 2 Ask Volume',\n",
    "        'Level 3 Bid Price',\n",
    "        'Level 3 Bid Volume',\n",
    "        'Level 3 Ask Price', \n",
    "        'Level 3 Ask Volume',\n",
    "        'Level 4 Bid Price',\n",
    "        'Level 4 Bid Volume',\n",
    "        'Level 4 Ask Price', \n",
    "        'Level 4 Ask Volume',\n",
    "        'Level 5 Bid Price',\n",
    "        'Level 5 Bid Volume',\n",
    "        'Level 5 Ask Price', \n",
    "        'Level 5 Ask Volume',\n",
    "        'midPrice'\n",
    "    ]\n",
    "    \n",
    "    target_column = 'midPrice'\n",
    "    \n",
    "    print(f\"\\nSelected Features: {feature_columns}\")\n",
    "    print(f\"Target Column: {target_column}\")\n",
    "    \n",
    "    feature_data = df[feature_columns].values\n",
    "    \n",
    "    if np.isnan(feature_data).any():\n",
    "        print(\"Warning: NaN values found in feature data. Forward-filling them.\")\n",
    "        feature_data = pd.DataFrame(feature_data, columns=feature_columns).fillna(method='ffill').values\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    SEQ_LEN = 300\n",
    "    X_train, y_train, X_test, y_test = preprocess(scaled_features, SEQ_LEN, train_split = 0.9)\n",
    "\n",
    "    DROPOUT = 0.2\n",
    "    WINDOW_SIZE = SEQ_LEN - 10\n",
    "    N_FEATURES = scaled_features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Architecture and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(WINDOW_SIZE, return_sequences=True, input_shape=(WINDOW_SIZE, N_FEATURES)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(LSTM(WINDOW_SIZE, return_sequences=True))\n",
    "    model.add(Dropout(rate=DROPOUT))\n",
    "    model.add(LSTM(WINDOW_SIZE, return_sequences=False)) \n",
    "    model.add(Dropout(rate=DROPOUT))\n",
    "    model.add(Dense(units=N_FEATURES))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        min_lr=1e-3\n",
    "    )\n",
    "    \n",
    "    checkpoint_filepath = 'best_model.h5'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    adam = Adam(learning_rate=1e-4)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=[mean_absolute_percentage_error])\n",
    "    \n",
    "    BATCH_SIZE = 300\n",
    "\n",
    "    print(\"\\nStarting model training...\")\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=350,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False, \n",
    "            validation_split=0.2,\n",
    "            callbacks=[lr_scheduler, model_checkpoint_callback]\n",
    "        )\n",
    "        print(\"Model training finished.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        print(\"Attempting to continue with existing model...\")\n",
    "        # Create a dummy history object\n",
    "        history = type('obj', (object,), {\n",
    "            'history': {\n",
    "                'loss': [0.1],\n",
    "                'val_loss': [0.1]\n",
    "            }\n",
    "        })()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if os.path.exists(checkpoint_filepath):\n",
    "        print(f\"\\nLoading best model from {checkpoint_filepath} for evaluation.\")\n",
    "        best_model = keras.models.load_model(checkpoint_filepath, \n",
    "                                            custom_objects={'mean_absolute_percentage_error': mean_absolute_percentage_error})\n",
    "    else:\n",
    "        print(f\"\\nError: Best model not found at {checkpoint_filepath}. Using the last trained model.\")\n",
    "        best_model = model \n",
    "\n",
    "    print(\"\\nEvaluating model on test data...\")\n",
    "    test_loss, test_mape = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss (best model): {test_loss:.6f}\")\n",
    "    print(f\"Test MAPE (best model): {test_mape:.2f}%\")\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss Over Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('graphs/training_loss.png')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create training loss plot: {e}\")\n",
    "        print(\"Continuing with analysis...\")\n",
    "    \n",
    "    print(\"\\nMaking predictions on test data...\")\n",
    "    y_hat = best_model.predict(X_test)\n",
    "\n",
    "    y_test_inverse = scaler.inverse_transform(y_test)\n",
    "    y_hat_inverse = scaler.inverse_transform(y_hat)\n",
    "    \n",
    "    last_col_idx = feature_columns.index('midPrice') \n",
    "    y_test_last = y_test_inverse[:, last_col_idx]\n",
    "    y_hat_last = y_hat_inverse[:, last_col_idx]\n",
    "\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y_test_last, label=\"Actual Mid Price\", color='green', alpha=0.7)\n",
    "        plt.plot(y_hat_last, label=\"Predicted Mid Price\", color='red', alpha=0.7)\n",
    "\n",
    "        plt.title('Mid Price Prediction - Multivariate LSTM')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Price')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('graphs/prediction_comparison.png')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create prediction comparison plot: {e}\")\n",
    "        print(\"Continuing with analysis...\")\n",
    "    \n",
    "    mse = np.mean((y_test_last - y_hat_last) ** 2)\n",
    "    mae = np.mean(np.abs(y_test_last - y_hat_last))\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = numpy_mape(y_test_last, y_hat_last)\n",
    "\n",
    "    print(f\"\\nPrediction Metrics (on inverse transformed 'midPrice'):\")\n",
    "    print(f\"MSE: {mse:.6f}\")\n",
    "    print(f\"MAE: {mae:.6f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nFeature columns used for training: {feature_columns}\")\n",
    "    print(f\"Number of features: {N_FEATURES}\")\n",
    "\n",
    "    # Save results for the next step\n",
    "    results = {\n",
    "        'df': df,\n",
    "        'y_hat_last': y_hat_last,\n",
    "        'y_test_last': y_test_last,\n",
    "        'SEQ_LEN': SEQ_LEN,\n",
    "        'feature_columns': feature_columns,\n",
    "        'scaler': scaler,\n",
    "        'metrics': {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save results to file\n",
    "    with open('tensorflow_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(\"\\nTensorFlow results saved to 'tensorflow_results.pkl'\")\n",
    "    print(\"TensorFlow part completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
